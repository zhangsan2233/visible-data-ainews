import sqlite3
import json
import datetime
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import spacy
import math
from opencc import OpenCC
from urllib.parse import urlparse
import re
from collections import Counter
import os
from openai import OpenAI

# === é…ç½®å‚æ•° ===
DB_NAME = "sample.db"
OUTPUT_JSON = "news_graph_causal.json"
DEEPSEEK_API_KEY = "sk-f0f1640d654748f69a61d4eec3ec9192"  # æ›¿æ¢ä¸ºä½ çš„DeepSeek API Key

SHARED_CATEGORY_WEIGHT = 0.2
SHARED_ENTITY_WEIGHT = 0.6
TIME_DECAY_LAMBDA = 3
MIN_WEIGHT = 0.55
SAME_DOMAIN_WEIGHT = 0.05

# === åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ ===
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com"
)

# === ç®€ç¹è½¬æ¢å™¨ ===
cc = OpenCC('t2s')

print("ğŸ“– Loading news from DB...")

# === ä»æ•°æ®åº“åŠ è½½æ–°é—»ï¼ŒåŒ…æ‹¬ URL ===
conn = sqlite3.connect(DB_NAME)
cursor = conn.cursor()
cursor.execute("""
    SELECT id, title, description, publishedAt, categories, url
    FROM ai_news
    WHERE title IS NOT NULL AND description IS NOT NULL
""")
rows = cursor.fetchall()
conn.close()

print(f"âœ… Loaded {len(rows)} news records.")

# === æ„é€ èŠ‚ç‚¹ ===
nodes = []
news_data = []
for nid, title, desc, pub, cats, url in rows:
    title_s = cc.convert(title.strip())
    desc_s = cc.convert(desc.strip())
    pub_date = pub[:10] if pub else "unknown"
    categories_s = [cc.convert(c.strip()) for c in cats.split(",")] if cats else []

    news_data.append({
        "id": f"news_{nid}",
        "title": title_s,
        "summary": desc_s,
        "time": pub_date,
        "categories": categories_s,
        "url": url
    })
    nodes.append({
        "id": f"news_{nid}",
        "type": "news",
        "title": title_s,
        "summary": desc_s[:1000],
        "time": pub_date,
        "url": url
    })

# === æ–‡æœ¬å‘é‡æ¨¡å‹ ===
print("ğŸ§  Generating embeddings for title + summary...")
model = SentenceTransformer('all-MiniLM-L6-v2')
texts = [n["title"] + ". " + n["summary"] for n in news_data]
embeddings = model.encode(texts)

# === å®ä½“æå– ===
print("ğŸ” Extracting named entities...")
nlp = spacy.load("zh_core_web_sm")
entities_list = []
for n in news_data:
    doc = nlp(n["title"] + ". " + n["summary"])
    entities_list.append(set([ent.text for ent in doc.ents]))

# === æ„é€ è¾¹ ===
print("ğŸ”— Generating news-news relationships...")
links = []
for i in range(len(news_data)):
    for j in range(i + 1, len(news_data)):
        n1, n2 = news_data[i], news_data[j]

        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]

        # ç±»åˆ«å…±äº«ï¼ˆå¯¹æ•°ç¼©æ”¾ï¼‰
        cat_overlap = len(set(n1["categories"]) & set(n2["categories"]))
        cat_score = SHARED_CATEGORY_WEIGHT * math.log1p(cat_overlap)

        # å®ä½“å…±äº«ï¼ˆJaccardï¼‰
        ent_i, ent_j = entities_list[i], entities_list[j]
        if ent_i or ent_j:
            entity_jaccard = len(ent_i & ent_j) / len(ent_i | ent_j)
        else:
            entity_jaccard = 0
        entity_score = SHARED_ENTITY_WEIGHT * (entity_jaccard * 2.5)

        # æ—¶é—´è¡°å‡
        time_decay = 1.0
        try:
            d1 = datetime.date.fromisoformat(n1["time"])
            d2 = datetime.date.fromisoformat(n2["time"])
            days_diff = (d2 - d1).days
            if days_diff < 0:
                continue
            time_decay = math.exp(-abs(days_diff) / TIME_DECAY_LAMBDA)
        except:
            pass

        # åŒåŸŸååŠ æƒ
        domain_bonus = 0
        try:
            domain1 = urlparse(n1["url"]).netloc
            domain2 = urlparse(n2["url"]).netloc
            if domain1 and domain1 == domain2:
                domain_bonus = SAME_DOMAIN_WEIGHT
        except:
            pass

        # ç»¼åˆæƒé‡
        weight = (
            0.6 * sim +
            0.25 * entity_score +
            0.1 * cat_score
        ) * time_decay + domain_bonus

        if weight >= MIN_WEIGHT:
            links.append({
                "source": n1["id"],
                "target": n2["id"],
                "weight": round(weight, 2)
            })

print(f"âœ… Generated {len(links)} refined news-news relationships.")

# === æ£€æµ‹è¿é€šå›¾ï¼ˆç°‡ï¼‰===
print("ğŸ” Detecting connected components...")

def find_connected_components(nodes, links):
    """æ£€æµ‹å›¾ä¸­çš„è¿é€šåˆ†é‡"""
    graph = {}
    for node in nodes:
        graph[node["id"]] = []
    
    for link in links:
        graph[link["source"]].append(link["target"])
        graph[link["target"]].append(link["source"])
    
    visited = set()
    components = []
    
    for node_id in graph:
        if node_id not in visited:
            # BFSéå†è¿é€šåˆ†é‡
            component = []
            queue = [node_id]
            visited.add(node_id)
            
            while queue:
                current = queue.pop(0)
                component.append(current)
                for neighbor in graph[current]:
                    if neighbor not in visited:
                        visited.add(neighbor)
                        queue.append(neighbor)
            
            if len(component) > 1:  # åªä¿ç•™æœ‰è¿æ¥çš„ç°‡
                components.append(component)
    
    return components

# æ£€æµ‹è¿é€šåˆ†é‡
components = find_connected_components(nodes, links)
print(f"ğŸ“Š Found {len(components)} connected components")

# === DeepSeek API è°ƒç”¨å‡½æ•° ===
def call_deepseek_api(prompt, max_retries=3):
    """ä½¿ç”¨OpenAI SDKè°ƒç”¨DeepSeek API"""
    for attempt in range(max_retries):
        try:
            print(f"  ğŸ”„ è°ƒç”¨DeepSeek API (å°è¯• {attempt + 1}/{max_retries})...")
            
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿ç”¨ç®€æ´å‡†ç¡®çš„è¯­è¨€æ¦‚æ‹¬æ–°é—»ä¸»é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=150,
                stream=False
            )
            
            content = response.choices[0].message.content.strip()
            
            # æ¸…ç†è¾“å‡º
            content = re.sub(r'^["\']|["\']$', '', content)
            content = re.sub(r'\s+', ' ', content)
            
            print(f"  âœ… APIè°ƒç”¨æˆåŠŸ")
            return content
            
        except Exception as e:
            print(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(2)
            else:
                return None
    
    return None

def generate_cluster_summary_with_deepseek(component_nodes, news_data, cluster_idx):
    """ä½¿ç”¨DeepSeekç”Ÿæˆç°‡æ¦‚æ‹¬"""
    # æ”¶é›†ç°‡å†…æ–°é—»ä¿¡æ¯
    cluster_titles = []
    
    for node_id in component_nodes:
        node = next((n for n in news_data if n["id"] == node_id), None)
        if node:
            cluster_titles.append(node["title"])
    
    if not cluster_titles:
        return f"AIæ–°é—»é›†ç¾¤ {cluster_idx}"
    
    # æ„å»ºä¼˜åŒ–çš„æç¤ºè¯
    prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸€ç»„ç›¸å…³çš„AIæ–°é—»æ ‡é¢˜ï¼Œç”¨15-25å­—æ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜å’ŒæŠ€æœ¯ç„¦ç‚¹ï¼š

{chr(10).join([f"â€¢ {title}" for title in cluster_titles[:6]])}

è¯·ç”Ÿæˆä¸€ä¸ªä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡æ¦‚æ‹¬ï¼Œè¦æ±‚ï¼š
1. å‡†ç¡®åæ˜ æ ¸å¿ƒä¸»é¢˜å’ŒæŠ€æœ¯æ–¹å‘
2. 15-25å­—é•¿åº¦
3. çªå‡ºå…·ä½“æŠ€æœ¯åç§°å’Œå…³é”®åº”ç”¨
4. é¿å…é€šç”¨æè¿°

æ¦‚æ‹¬ï¼š"""
    
    summary = call_deepseek_api(prompt)
    
    if summary and len(summary) >= 10 and len(summary) <= 50:
        return summary
    else:
        # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        return generate_fallback_summary(cluster_titles, cluster_idx)

def generate_fallback_summary(titles, cluster_idx):
    """å¤‡ç”¨æ¦‚æ‹¬ç”Ÿæˆæ–¹æ¡ˆ"""
    all_text = " ".join(titles)
    
    # æå–å…³é”®è¯
    words = re.findall(r'[\u4e00-\u9fff]+|[A-Z][a-z]*[A-Z][a-zA-Z]*', all_text)
    
    # è¿‡æ»¤åœç”¨è¯
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š'}
    words = [word for word in words if word not in stop_words and len(word) > 1]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(words)
    top_keywords = [word for word, freq in word_freq.most_common(3)]
    
    # è¯†åˆ«æŠ€æœ¯æœ¯è¯­
    tech_terms = list(set(re.findall(r'\b[A-Z][a-z]*[A-Z][a-zA-Z]*\b', all_text)))
    
    if tech_terms and top_keywords:
        return f"{tech_terms[0]}æŠ€æœ¯ï¼š{', '.join(top_keywords[:2])}"
    elif tech_terms:
        return f"{tech_terms[0]}æŠ€æœ¯åˆ›æ–°ä¸å‘å±•"
    elif top_keywords:
        return f"AI{top_keywords[0]}é¢†åŸŸè¿›å±•ä¸åº”ç”¨"
    else:
        return f"AIæŠ€æœ¯å‰æ²¿é›†ç¾¤ {cluster_idx}"

def generate_global_summary_with_deepseek(news_data):
    """ä½¿ç”¨DeepSeekç”Ÿæˆå…¨å±€æ‘˜è¦"""
    sample_titles = [n["title"] for n in news_data[:12]]
    
    prompt = f"""åŸºäºä»¥ä¸‹AIæ–°é—»æ ‡é¢˜ï¼Œç”¨20-30å­—æ¦‚æ‹¬å½“å‰AIé¢†åŸŸçš„ä¸»è¦è¶‹åŠ¿å’Œå‘å±•åŠ¨æ€ï¼š

{chr(10).join([f"â€¢ {title}" for title in sample_titles])}

è¯·ç”Ÿæˆä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šçš„ä¸­æ–‡æ¦‚æ‹¬ï¼Œæè¿°æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œè¡Œä¸šçƒ­ç‚¹ï¼š"""
    
    summary = call_deepseek_api(prompt, max_retries=2)
    
    if summary:
        return summary
    else:
        # å¤‡ç”¨å…¨å±€æ‘˜è¦
        return "AIæŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå¤§æ¨¡å‹ã€ç®—åŠ›ä¼˜åŒ–ä¸åˆ›æ–°åº”ç”¨æˆä¸ºè¡Œä¸šç„¦ç‚¹ï¼Œå„é¢†åŸŸè¿æ¥æŠ€æœ¯çªç ´"

# === ä¸ºæ¯ä¸ªè¿é€šå›¾ç”ŸæˆDeepSeekæ¦‚æ‹¬ ===
print("ğŸ§© Generating cluster summaries with DeepSeek API...")
cluster_summaries = {}

# ç”±äºæœ‰41ä¸ªç°‡ï¼Œæˆ‘ä»¬åªå¤„ç†å‰20ä¸ªä»¥é¿å…è¿‡å¤šAPIè°ƒç”¨
max_clusters_to_process = min(20, len(components))
successful_api_calls = 0

for idx, component in enumerate(components):
    if len(component) < 2:
        continue
        
    if idx >= max_clusters_to_process:
        # å¯¹äºè¶…è¿‡é™åˆ¶çš„ç°‡ï¼Œä½¿ç”¨ç®€å•å‘½å
        cluster_id = f"cluster_{idx}"
        cluster_summaries[cluster_id] = {
            "summary": f"AIæ–°é—»é›†ç¾¤ {idx}",
            "node_ids": component,
            "size": len(component)
        }
        continue
        
    print(f"  ğŸ“ å¤„ç†ç°‡ {idx} ({len(component)} ä¸ªèŠ‚ç‚¹)...")
    
    # ä½¿ç”¨DeepSeekç”Ÿæˆæ¦‚æ‹¬
    summary = generate_cluster_summary_with_deepseek(component, news_data, idx)
    cluster_id = f"cluster_{idx}"
    cluster_summaries[cluster_id] = {
        "summary": summary,
        "node_ids": component,
        "size": len(component)
    }
    
    # ç»Ÿè®¡æˆåŠŸè°ƒç”¨
    if not any(word in summary for word in ["é›†ç¾¤", "é›†ç¾¤"]):
        successful_api_calls += 1
    
    print(f"  âœ… ç°‡ {idx}: {summary}")
    
    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
    import time
    time.sleep(1)

# === å…¨å±€è¯­ä¹‰æ‘˜è¦ ===
print("ğŸ§© Generating global semantic summary with DeepSeek API...")
semantic_summary = generate_global_summary_with_deepseek(news_data)

# === æ·»åŠ  summary èŠ‚ç‚¹ ===
summary_node = {
    "id": "summary_global",
    "type": "summary",
    "title": "æ–°é—»è¯­ä¹‰æ¦‚æ‹¬",
    "summary": semantic_summary,
    "time": "N/A",
    "url": "",
    "color": "#FFD700"
}
nodes.append(summary_node)

# === è¿æ¥ summary èŠ‚ç‚¹åˆ°å‰ 8 æ¡æ–°é—»èŠ‚ç‚¹ ===
for n in news_data[:8]:
    links.append({
        "source": "summary_global",
        "target": n["id"],
        "weight": 0.3
    })

# === è¾“å‡º JSONï¼ŒåŒ…å«ç°‡æ¦‚æ‹¬ä¿¡æ¯ ===
graph_data = {
    "force_graph": {"nodes": nodes, "links": links},
    "semantic_summary": semantic_summary,
    "cluster_summaries": cluster_summaries
}

with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(graph_data, f, ensure_ascii=False, indent=2)

print(f"ğŸ¯ Graph with DeepSeek-generated cluster summaries saved to {OUTPUT_JSON}")
print(f"ğŸ“Š Cluster info: {len(cluster_summaries)} clusters with summaries")
print(f"ğŸ”— API Success: {successful_api_calls}/{max_clusters_to_process} clusters used DeepSeek API")
print(f"ğŸ§  Global Summary: {semantic_summary}")